{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test 6.1 neuron logloss.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWTTy4lrZca5"
      },
      "source": [
        "# Обучение нейрона с помощью функции потерь LogLoss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_zlIW5BZcbB"
      },
      "source": [
        "<h3 style=\"text-align: center;\"><b>Нейрон с сигмоидой</b></h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWwW32pCZcbC"
      },
      "source": [
        "Снова рассмотрим нейрон с сигмоидой, то есть $$f(x) = \\sigma(x)=\\frac{1}{1+e^{-x}}$$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "mj5QizQIZcbC"
      },
      "source": [
        "Ранее мы установили, что **обучение нейрона с сигмоидой с квадратичной функцией потерь**:  \n",
        "\n",
        "$$MSE(w, x) = \\frac{1}{2n}\\sum_{i=1}^{n} (\\hat{y_i} - y_i)^2 = \\frac{1}{2n}\\sum_{i=1}^{n} (\\sigma(w \\cdot x_i) - y_i)^2$$    \n",
        "\n",
        "где $w \\cdot x_i$ - скалярное произведение, а $\\sigma(w \\cdot x_i) =\\frac{1}{1+e^{-w \\cdot x_i}} $ - сигмоида -- **неэффективно**, то есть мы увидели, что даже за большое количество итераций нейрон предсказывает плохо."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTgNbravZcbE"
      },
      "source": [
        "Давайте ещё раз взглянем на формулу для градиентного спуска от функции потерь $MSE$ по весам нейрона:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "RHXjByowZcbE"
      },
      "source": [
        "$$ \\frac{\\partial MSE}{\\partial w} = \\frac{1}{n} X^T (\\sigma(w \\cdot X) - y)\\sigma(w \\cdot X)(1 - \\sigma(w \\cdot X))$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebNqrTV3ZcbF"
      },
      "source": [
        "А теперь смотрим на график сигмоиды:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "366q3EzJZcbH"
      },
      "source": [
        "<img src=\"https://cdn-images-1.medium.com/max/1200/1*IDAnCFoeXqWL7F4u9MJMtA.png\" width=500px height=350px>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSEF3EWVZcbJ"
      },
      "source": [
        "**Её значения: числа от 0 до 1.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oOJPcKhZcbJ"
      },
      "source": [
        "Если получше проанализировать формулу, то теперь можно заметить, что, поскольку сигмоида принимает значения между 0 и 1 (а значит (1-$\\sigma$) тоже принимает значения от 0 до 1), то мы умножаем $X^T$ на столбец $(\\sigma(w \\cdot X) - y)$ из чисел от -1 до 1, а потом ещё на столбцы $\\sigma(w \\cdot X)$ и $(1 - \\sigma(w \\cdot X))$ из чисел от 0 до 1. Таким образом в лучшем случае $\\frac{\\partial{Loss}}{\\partial{w}}$ будет столбцом из чисел, порядок которых максимум 0.01 (в среднем, понятно, что если сигмоида выдаёт все 0, то будет 0, если все 1, то тоже 0). После этого мы умножаем на шаг градиентного спуска, который обычно порядка 0.001 или 0.1 максимум. То есть мы вычитаем из весов числа порядка ~0.0001. Медленновато спускаемся, не правда ли? Это называют **проблемой затухающих градиентов**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHmUqm5xZcbK"
      },
      "source": [
        "Чтобы избежать эту проблему в задачах классификации, в которых моделью является нейрон с сигмоидной функцией активации, предсказывающий \"вероятности\" принадлженостей к классамиспользуют **LogLoss**:  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dbw69trJZcbL"
      },
      "source": [
        "$$J(\\hat{y}, y) = -\\frac{1}{n} \\sum_{i=1}^n y_i \\log(\\hat{y_i}) + (1 - y_i) \\log(1 - \\hat{y_i}) = -\\frac{1}{n} \\sum_{i=1}^n y_i \\log(\\sigma(w \\cdot x_i)) + (1 - y_i) \\log(1 - \\sigma(w \\cdot x_i))$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXcBIx73ZcbM"
      },
      "source": [
        "где, как и прежде, $y$ - столбец $(n, 1)$ из истинных значений классов, а $\\hat{y}$ - столбец $(n, 1)$ из предсказаний нейрона."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DBEOhJmZcbN"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_x5CX0RyZcbQ"
      },
      "source": [
        "def loss(y_pred, y):\n",
        "    return -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raPTihU6ZcbT"
      },
      "source": [
        "Отметим, что сейчас речь идёт именно о **бинарной классификации (на два класса)**, в многоклассовой классификации используется функция потерь под названием *кросс-энтропия*, которая является обобщением LogLoss'а на случай нескольких классов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0Ky5xUfvOzh"
      },
      "source": [
        "Почему же теперь всё будет лучше? Раньше была проблема умножения маленьких чисел в градиенте. Давайте посмотрим, что теперь:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxs0zXfPvOzh"
      },
      "source": [
        "* Для веса $w_j$:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hH6AujkCvOzj"
      },
      "source": [
        "$$ \\frac{\\partial Loss}{\\partial w_j} = \n",
        "-\\frac{1}{n} \\sum_{i=1}^n \\left(\\frac{y_i}{\\sigma(w \\cdot x_i)} - \\frac{1 - y_i}{1 - \\sigma(w \\cdot x_i)}\\right)(\\sigma(w \\cdot x_i))_{w_j}' = -\\frac{1}{n} \\sum_{i=1}^n \\left(\\frac{y_i}{\\sigma(w \\cdot x_i)} - \\frac{1 - y_i}{1 - \\sigma(w \\cdot x_i)}\\right)\\sigma(w \\cdot x_i)(1 - \\sigma(w \\cdot x_i))x_{ij} = $$\n",
        "$$-\\frac{1}{n} \\sum_{i=1}^n \\left(y_i - \\sigma(w \\cdot x_i)\\right)x_{ij}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYOygL-zvOzk"
      },
      "source": [
        "* Градиент $Loss$'а по вектору весов -- это вектор, $j$-ая компонента которого равна $\\frac{\\partial Loss}{\\partial w_j}$ (помним, что весов всего $m$):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItJoyReVvOzl"
      },
      "source": [
        "$$\\begin{align}\n",
        "    \\frac{\\partial Loss}{\\partial w} &= \\begin{bmatrix}\n",
        "           -\\frac{1}{n} \\sum_{i=1}^n \\left(y_i - \\sigma(w \\cdot x_i)\\right)x_{i1} \\\\\n",
        "           -\\frac{1}{n} \\sum_{i=1}^n \\left(y_i - \\sigma(w \\cdot x_i)\\right)x_{i2} \\\\\n",
        "           \\vdots \\\\\n",
        "           -\\frac{1}{n} \\sum_{i=1}^n \\left(y_i - \\sigma(w \\cdot x_i)\\right)x_{im}\n",
        "         \\end{bmatrix}\n",
        "\\end{align}=\\frac{1}{n} X^T \\left(\\hat{y} - y\\right)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2xKo6NzvOzm"
      },
      "source": [
        "По аналогии с $w_j$ выведите формулу для свободного члена (bias'а) $b$ (*hint*: можно считать, что при нём есть признак $x_{i0}=1$ на всех $i$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HQALb8XZcbU"
      },
      "source": [
        "Получили новое правило для обновления $w$ и $b$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xL2gqg27ZcbW"
      },
      "source": [
        "def sigmoid(x):\n",
        "    \"\"\"Сигмоидальная функция\"\"\"\n",
        "    return 1 / (1 + np.exp(-x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukZ-iJNRZcbY"
      },
      "source": [
        "Реализуйте нейрон с функцией потерь LogLoss:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdnkoJDfZcba"
      },
      "source": [
        "class Neuron:\n",
        "    \n",
        "    def __init__(self, w=None, b=0):\n",
        "        \"\"\"\n",
        "        :param: w -- вектор весов\n",
        "        :param: b -- смещение\n",
        "        \"\"\"\n",
        "        # пока что мы не знаем размер матрицы X, а значит не знаем, сколько будет весов\n",
        "        self.w = w\n",
        "        self.b = b\n",
        "        \n",
        "        \n",
        "    def activate(self, x):\n",
        "        return sigmoid(x)\n",
        "    \n",
        "        \n",
        "    def forward_pass(self, X):\n",
        "        \"\"\"\n",
        "        Эта функция рассчитывает ответ нейрона при предъявлении набора объектов\n",
        "        :param: X -- матрица объектов размера (n, m), каждая строка - отдельный объект\n",
        "        :return: вектор размера (n, 1) из нулей и единиц с ответами перцептрона \n",
        "        \"\"\"\n",
        "        # реализуйте forward_pass\n",
        "        \n",
        "    \n",
        "    def backward_pass(self, X, y, y_pred, learning_rate=0.1):\n",
        "        \"\"\"\n",
        "        Обновляет значения весов нейрона в соответствие с этим объектом\n",
        "        :param: X -- матрица объектов размера (n, m)\n",
        "                y -- вектор правильных ответов размера (n, 1)\n",
        "                learning_rate - \"скорость обучения\" (символ alpha в формулах выше)\n",
        "        В этом методе ничего возвращать не нужно, только правильно поменять веса\n",
        "        с помощью градиентного спуска.\n",
        "        \"\"\"\n",
        "        # тут нужно обновить веса по формулам, написанным выше\n",
        "        \n",
        "    \n",
        "    def fit(self, X, y, num_epochs=5000):\n",
        "        \"\"\"\n",
        "        Спускаемся в минимум\n",
        "        :param: X -- матрица объектов размера (n, m)\n",
        "                y -- вектор правильных ответов размера (n, 1)\n",
        "                num_epochs -- количество итераций обучения\n",
        "        :return: J_values -- вектор значений функции потерь\n",
        "        \"\"\"\n",
        "        self.w = np.zeros((X.shape[1], 1))  # столбец (m, 1)\n",
        "        self.b = 0  # смещение\n",
        "        loss_values = []  # значения функции потерь на различных итерациях обновления весов\n",
        "        \n",
        "        for i in range(num_epochs):\n",
        "            # предсказания с текущими весами\n",
        "            y_pred = self.forward_pass(X)\n",
        "            # считаем функцию потерь с текущими весами\n",
        "            loss_values.append(loss(y_pred, y))\n",
        "            # обновляем веса по формуле градиентного спуска\n",
        "            self.backward_pass(X, y, y_pred)\n",
        "\n",
        "        return loss_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RQRPI50Zcbb"
      },
      "source": [
        "<h3 style=\"text-align: center;\"><b>Тестирование</b></h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBwpsQc0Zcbc"
      },
      "source": [
        "Протестируем нейрон, обученный с новой функцией потерь, на тех же данных, что и в предыдущем ноутбуке:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVCPAAuXZcbd"
      },
      "source": [
        "**Проверка forward_pass()**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbuAzv_YZcbe"
      },
      "source": [
        "w = np.array([1., 2.]).reshape(2, 1)\n",
        "b = 2.\n",
        "X = np.array([[1., 3.],\n",
        "              [2., 4.],\n",
        "              [-1., -3.2]])\n",
        "\n",
        "neuron = Neuron(w, b)\n",
        "y_pred = neuron.forward_pass(X)\n",
        "print (\"y_pred = \" + str(y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eL6viEHNZcbi"
      },
      "source": [
        "**Проверка backward_pass()**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4F8zActZcbk"
      },
      "source": [
        "y = np.array([1, 0, 1]).reshape(3, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILDcoLqwZcbm"
      },
      "source": [
        "neuron.backward_pass(X, y, y_pred)\n",
        "\n",
        "print (\"w = \" + str(neuron.w))\n",
        "print (\"b = \" + str(neuron.b))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1ZRdKcfZcbq"
      },
      "source": [
        "Проверьте на наборах данных \"яблоки и груши\" и \"голос\"."
      ]
    }
  ]
}