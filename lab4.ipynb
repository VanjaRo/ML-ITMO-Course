{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DT_test.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGClrhQA9SAk"
      },
      "source": [
        "# Деревья решений"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veekMy8WRjBi"
      },
      "source": [
        "## Построение дерева"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYkVwAFiUHXj"
      },
      "source": [
        "Опишем жадный алгоритм построения бинарного дерева решений:\n",
        "1. Начинаем со всей обучающей выборки $X$, которую помещаем в корень $R_1$. \n",
        "2. Задаём функционал качества $Q(X, j, t)$ и критерий остановки. \n",
        "3. Запускаем построение из корня: $SplitNode(1, R_1)$\n",
        "\n",
        "Функция $SplitNode(m, R_m)$\n",
        "1. Если выполнен критерий остановки, то выход.\n",
        "2. Находим наилучший с точки зрения $Q$ предикат: $j, t$: $[x_j<t]$\n",
        "3. Помещаем предикат в вкршину и получаем с его помощью разбиение $X$ на две части: $R_{left} = \\lbrace x|x_j<t \\rbrace$ и $R_{right} = \\lbrace x|x_j \\geqslant t \\rbrace$\n",
        "4. Поместим $R_{left}$ и $R_{right}$ соответсвенно в левое и правое поддерево.\n",
        "5. Рекурсивно повторяем $SplitNode(left, R_{left})$ и $SplitNode(right, R_{right})$.\n",
        "\n",
        "В конце поставим в соответствие каждому листу ответ. Для задачи классификации - это самый частый среди объектов класс или вектор с долями классов (можно интерпретировать как вероятности):\n",
        "$$ c_v = \\arg \\max_{k\\in Y} \\sum_{(x_i,y_i) \\in R_v} [y_i=k]  $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9P6FsdBog4Ai"
      },
      "source": [
        "## Функционал качества для деревьев решений\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VAKO0aykGBD"
      },
      "source": [
        "Энтропия Шеннона для системы с N возможными состояниями определяется по формуле:\n",
        "$$H = - \\sum_{i=0}^{N} p_i\\log_2p_i $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5582B-1Fn2bw"
      },
      "source": [
        "где $p_i$ – вероятности нахождения системы в $i$-ом состоянии. \n",
        "\n",
        "Это очень важное понятие теории информации, которое позволяет оценить количество информации (степень хаоса в системе). Чем выше энтропия, тем менее упорядочена система и наоборот. С помощью энтропии мы формализуем функционал качества для разделение выборки (для задачи классификации)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbcMUd7bvk05"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import random\n",
        "from collections import Counter\n",
        "from pprint import pprint\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AdLxP9CowTm"
      },
      "source": [
        "Код для расчёта энтропии:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mT8Jq8Av2sM"
      },
      "source": [
        "def entropy(y):\n",
        "    \n",
        "    _, counts = np.unique(y, return_counts=True)\n",
        "\n",
        "    probabilities = counts / counts.sum()\n",
        "    entropy = sum(probabilities * -np.log2(probabilities))\n",
        "     \n",
        "    return entropy"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xk9etb2vo7fK"
      },
      "source": [
        "Здесь $y$ - это массив значений целевой переменной"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07TCw0USzLus"
      },
      "source": [
        "Энтропия – по сути степень хаоса (или неопределенности) в системе. Уменьшение энтропии называют приростом информации (information gain, IG).\n",
        "\n",
        "Обочначим $R_v$ - объекты, которые нужно разделить в помощью предиката в вершине $v$. Запишем формулу для расчёта информационного прироста:\n",
        "$$ Q = IG = H(R_v) - (H(R_{left})+H(R_{right}))$$\n",
        "\n",
        "На каждом шаге нам нужно максимизировать этот функционал качества. Как это делать? Например, так можно перебрать $t$ для выбранного $j$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trEWHDoXg_p9"
      },
      "source": [
        "Предыдущая версия формулы прироста информации слишком упрощена. В работе необходимо использовать более устойчивую формулу, которая учитывает не только энтропию подмножеств, но и их размер. \n",
        "\n",
        "$$ Q = IG = H(R_v) - \\Big (\\frac{|R_{left}|} {|R_{v}|} H(R_{left})+ \\frac{|R_{right}|} {|R_{v}|} H(R_{right})\\Big)$$\n",
        "\n",
        "где, $|R_{v}|$, $|R_{left}|$ и $|R_{right}|$ - количество элементов в соответствующих множествах."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xmN6V_N1xBr"
      },
      "source": [
        "\n",
        "### Задание 4.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWFHZScF2CBF"
      },
      "source": [
        "Реализуйте алгоритм построения дерева. Должны быть отдельные функции (методы) для расчёта энтропии (уже есть), для разделения дерева (используйте `pandas`), для подсчёта функционала качества $IG$, для выбора наилучшего разделения (с учетом признакоd и порогов), для проверки критерия остановки.\n",
        "\n",
        "Для набора данных `iris` реализуйте алгоритм и минимум три из разными критерия остановки из перечисленных ниже:\n",
        "* максимальной глубины дерева = 5\n",
        "* минимального числа объектов в листе = 5\n",
        "* максимальное количество листьев в дереве = 5\n",
        "* purity (остановка, если все объекты в листе относятся к одному классу)\n",
        "\n",
        "Реализуйте функцию `predict` (на вход функции подаётся датафрейм с объектами)\n",
        "\n",
        "Оцените точность каждой модели с помощью метрики точность (`from sklearn.metrics import accuracy_score` или реализовать свою)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40uW5paXeDLL"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, feature=None, threshold=None, data_left=None, data_right=None, gain=None, value=None):\n",
        "        self.feature = feature\n",
        "        self.threshold = threshold\n",
        "        self.data_left = data_left\n",
        "        self.data_right = data_right\n",
        "        self.gain = gain\n",
        "        self.value = value\n",
        "\n",
        "class DecisionTree:\n",
        "  def __init__(self, H, max_depth=5, min_samples_split=5, max_leaves=5):\n",
        "    self.max_depth = max_depth\n",
        "    self.min_samples_split = min_samples_split\n",
        "    self.max_leaves = max_leaves\n",
        "    self.leaves_num = 0\n",
        "    self.H = H\n",
        "    self.root = None\n",
        "  \n",
        "  def _information_gain(self, parent, left_c, right_c):\n",
        "    left_ratio = len(left_c) / len(parent)\n",
        "    right_ratio = len(right_c) / len(parent)\n",
        "    return self.H(parent) - (left_ratio * self.H(left_c) + right_ratio * self.H(right_c))\n",
        "  \n",
        "  def _best_split(self, X, y):\n",
        "    best_split = {}\n",
        "    best_info_gain = -1\n",
        "\n",
        "    # feature index\n",
        "    df = pd.concat([X, y], axis=1)\n",
        "\n",
        "    for feature in X:\n",
        "      X_curr = X[feature]\n",
        "      \n",
        "      for threshold in X_curr:\n",
        "        df_left = df[df[feature] <= threshold]\n",
        "        df_right = df[df[feature] > threshold]\n",
        "\n",
        "        if len(df_left) > 0 and len(df_right) > 0:\n",
        "          y = df.iloc[:, -1]\n",
        "          y_left = df_left.iloc[:, -1]\n",
        "          y_right = df_right.iloc[:, -1]\n",
        "\n",
        "          gain = self._information_gain(y, y_left, y_right)\n",
        "          if gain > best_info_gain:\n",
        "            best_split = {\n",
        "                'feature': feature,\n",
        "                'threshold': threshold,\n",
        "                'df_left': df_left,\n",
        "                'df_right': df_right,\n",
        "                'gain': gain\n",
        "            }\n",
        "            best_info_gain = gain\n",
        "    return best_split\n",
        "  \n",
        "  def _build(self, X, y, depth=0):\n",
        "    self.leaves_num -= 1\n",
        "    n_rows, n_cols = X.shape\n",
        "\n",
        "    if n_rows >= self.min_samples_split and depth <= self.max_depth and self.leaves_num + 2 <= self.max_leaves:\n",
        "      self.leaves_num += 2\n",
        "      best = self._best_split(X, y)\n",
        "\n",
        "      if best['gain'] > 0:\n",
        "        left = self._build(\n",
        "            X = best['df_left'].iloc[:,:-1],\n",
        "            y = best['df_left'].iloc[:,-1],\n",
        "            depth = depth + 1\n",
        "        )\n",
        "        right = self._build(\n",
        "            X = best['df_right'].iloc[:,:-1],\n",
        "            y = best['df_right'].iloc[:,-1],\n",
        "            depth = depth + 1\n",
        "        )\n",
        "        return Node(\n",
        "            feature = best['feature'],\n",
        "            threshold = best['threshold'],\n",
        "            data_left = left,\n",
        "            data_right = right,\n",
        "            gain = best['gain']\n",
        "        )\n",
        "\n",
        "    # return leaf\n",
        "    self.leaves_num += 1\n",
        "    return Node(\n",
        "        value = Counter(y).most_common(1)[0][0]\n",
        "    )\n",
        "\n",
        "  @staticmethod\n",
        "  def _add_map(a, b):\n",
        "    for key_b in b:\n",
        "      if key_b  not in a:\n",
        "        a[key_b] = 0\n",
        "      a[key_b] += b[key_b]\n",
        "    return a\n",
        "\n",
        "  def _feature_traverse(self, node, features):\n",
        "    if node.feature == None or node.gain == None:\n",
        "      return features \n",
        "\n",
        "    if node.feature not in features:\n",
        "      features[node.feature] = 0\n",
        "    features[node.feature] += node.gain\n",
        "\n",
        "    if node.data_left != None:\n",
        "      features = self._add_map(features,\n",
        "                               self._feature_traverse(node.data_left, features))\n",
        "    if node.data_right != None:\n",
        "      features = self._add_map(features,\n",
        "                               self._feature_traverse(node.data_right, features))\n",
        "    return features\n",
        "    \n",
        "\n",
        "  def feature_importance(self):\n",
        "    features = {}\n",
        "    features = self._feature_traverse(self.root, features)\n",
        "    feat_sum = sum(features.values())\n",
        "    for f_key in features:\n",
        "      features[f_key] /= feat_sum\n",
        "\n",
        "    return features\n",
        "\n",
        "  \n",
        "  def fit(self, X, y):\n",
        "    self.root = self._build(X, y)\n",
        "  \n",
        "  def _predict_one(self, x, tree):\n",
        "    if tree.value != None:\n",
        "      return tree.value\n",
        "    feature_value = x[tree.feature]\n",
        "\n",
        "    if feature_value <= tree.threshold:\n",
        "      return self._predict_one(x, tree.data_left)\n",
        "\n",
        "    if feature_value > tree.threshold:\n",
        "      return self._predict_one(x, tree.data_right)\n",
        "\n",
        "  def predict(self, X):\n",
        "    return [self._predict_one(X.iloc[i], self.root) for i in range(len(X))]\n"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nndxvrbXYEMg"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris = load_iris()\n",
        "data_ir = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
        "                     columns= iris['feature_names'] + ['target'])\n",
        "X_iris = data_ir.iloc[:, :-1]\n",
        "y_iris = data_ir.iloc[:, -1]\n"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yYXrKJIZX6d",
        "outputId": "b977cb13-e2bc-4e97-b727-a01990842bce"
      },
      "source": [
        "\n",
        "dt_1.feature_importance())\n"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'petal length (cm)': 0.913022262807127,\n",
              " 'petal width (cm)': 0.07271248001777207,\n",
              " 'sepal width (cm)': 0.014265257175101088}"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5vnC7HXYZdW"
      },
      "source": [
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_iris, y_iris, random_state=42, test_size=0.2)\n"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exzsJtaaY6Pd",
        "outputId": "6c2fb20d-8919-412b-89a6-978360c86e51"
      },
      "source": [
        "dt_1 = DecisionTree(entropy, max_leaves=15)\n",
        "dt_1.fit(X_train, y_train)\n",
        "preds_1 = dt_1.predict(X_test)\n",
        "print(accuracy_score(y_test, preds_1))\n",
        "\n",
        "dt_2 = DecisionTree(entropy, max_depth=6)\n",
        "dt_2.fit(X_train, y_train)\n",
        "preds_2 = dt_2.predict(X_test)\n",
        "print(accuracy_score(y_test, preds_2))\n",
        "\n",
        "dt_3 = DecisionTree(entropy, min_samples_split=100)\n",
        "dt_3.fit(X_train, y_train)\n",
        "preds_3 = dt_3.predict(X_test)\n",
        "print(accuracy_score(y_test, preds_3))\n",
        "\n"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n",
            "0.9666666666666667\n",
            "0.6333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BpNrqC8GsCX",
        "outputId": "aeaf07c0-5592-4fee-863b-5853a6dbfcec"
      },
      "source": [
        "print(type(X_train))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkyCjLcy_CTM"
      },
      "source": [
        "##  Случайный лес"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fKZe1FyRgCa"
      },
      "source": [
        "Опишем алгоритм случайный лес (*random forest*) и попутно разберём основные идеи:\n",
        "\n",
        "1. Зададим $N$ - число деревьев в лесу.\n",
        "2. Для каждого $n$ из $N$ сгенерируем свою выборку $X_n$. Пусть $m$ - это количество объектов в $X$. При генерации каждой $X_n$ мы будем брать объекты $m$ раз с возвращением. То есть один и тот же объект может попасть в выборку несколько раз, а какие-то объекты не попадут. (Этот способ назвается бутстрап).\n",
        "3. По каждой $X_n$ построим решающее дерево $b_n$. Обычно стараются делать глубокие деревья. В качестве критериев остановки можно использовать `max_depth` или `min_samples_leaf` (например, пока в каждом листе не окажется по одному объекту). При каждом разбиении сначала выбирается $k$ (эвристика $k = \\sqrt d$, где $d$ - это число признаков объектов из выборки $X$) случайных признаков из исходных, и оптимальное разделение выборки ищется только среди них. Обратите внимание, что мы не выбрасываем оставшиеся признаки!\n",
        "4. Итоговый алгоритм будет представлять собой результат голосования (для классификации) и среднее арифметическое (для регрессии). Модификация алгоритма предполагает учёт весов каждого отдельного слабого алгоритма в ансамбле, но в этом особо нет смысла.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJBQ8lc0WyrN"
      },
      "source": [
        "### Задание 4.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y594Jn04ZTCm"
      },
      "source": [
        "В качестве набора данных используйте: https://www.kaggle.com/mathchi/churn-for-bank-customers\n",
        "\n",
        "Там есть описание и примеры работы с этими данными. Если кратко, речь идёт про задачу прогнозирования оттока клиентов. Есть данные о 10 тысячах клиентов банка, часть из которых больше не являются клиентами."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be_mLbdVW2oG"
      },
      "source": [
        "Используя либо свою реализацию, либо  `DecisionTreeClassifier` с разными настройками из `sklearn.tree` реализйте алгоритм \"случайный лес\". \n",
        "\n",
        "Найдите наилучшие гиперпараметры этого алгоритма: количество деревьев, критерий остановки, функционал качества, минимальное количество объектов в листьях и другие.\n",
        "\n",
        "Нельзя использовать готовую реализацию случайного леса из `sklearn`.\n",
        "\n",
        "В подобных задачах очень важна интерпретируемость алгоритма. Попытайтесь оценить информативность признаков, т.е. ответить а вопрос, значения каких признаков являются самыми важными индикаторами того, что банк потеряет клиента."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9014RB_FFxb"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6rSdZZV3rId"
      },
      "source": [
        "class RandomForest:\n",
        "  def __init__(self, random_state=42, trees_amount=20, min_samples_split=10, max_depth=5):\n",
        "    self.trees_amount = trees_amount\n",
        "    self.min_samples_split = min_samples_split\n",
        "    self.max_depth = max_depth\n",
        "    self.random_state = random_state\n",
        "    np.random.seed(random_state)\n",
        "\n",
        "\n",
        "    self.decision_trees = []\n",
        "\n",
        "  @staticmethod\n",
        "  def _sample_boostrap(X, y):\n",
        "    n_rows, n_cols = X.shape\n",
        "    k = round(np.sqrt(n_cols))\n",
        "    samples = np.random.choice(n_rows, size=n_rows, replace=True)\n",
        "    \n",
        "    # deprecated\n",
        "    # k_samples = np.random.choice(X.columns, size=k, replace=False)\n",
        "    # return X[k_samples].iloc[samples], y.iloc[samples]\n",
        "\n",
        "    return X.iloc[samples], y.iloc[samples]\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    # refresh given forest if rebuilt\n",
        "    if len(self.decision_trees) > 0:\n",
        "      self.decision_trees = []\n",
        "    num_trees = 0\n",
        "    while num_trees < self.trees_amount:\n",
        "      # try:\n",
        "      dt = DecisionTreeClassifier(criterion=\"gini\",\n",
        "                        random_state=self.random_state,\n",
        "                        min_samples_split=self.min_samples_split,\n",
        "                        max_depth=self.max_depth,\n",
        "                        max_features=\"sqrt\"\n",
        "                        )\n",
        "      X_smpl, y_smpl = self._sample_boostrap(X, y)\n",
        "\n",
        "      dt.fit(X_smpl, y_smpl)\n",
        "      self.decision_trees.append(dt)\n",
        "\n",
        "      num_trees += 1\n",
        "      # except Exception:\n",
        "      #   continue\n",
        "\n",
        "  def predict(self, X):\n",
        "      y = []\n",
        "      for tree in self.decision_trees:\n",
        "        y.append(tree.predict(X))\n",
        "      \n",
        "      # so all the predictions to one data row would be in a common list\n",
        "      y = np.swapaxes(y, 0, 1)\n",
        "      \n",
        "      predictions = []\n",
        "      for preds in y:\n",
        "        counter = Counter(preds)\n",
        "        predictions.append(counter.most_common(1)[0][0])\n",
        "      return predictions\n",
        "  \n",
        "  def feature_importance(self, original_score, X_test, y_test):\n",
        "    predictions = []\n",
        "    for column in X_test:\n",
        "      X_cop = X_test.copy()\n",
        "      X_cop[column] = np.random.permutation(X_cop[column].values)\n",
        "      pred_score = accuracy_score(y_test, self.predict(X_cop))\n",
        "      predictions.append(((pred_score - original_score) / original_score, column))\n",
        "    # print(predictions)\n",
        "    predictions = sorted(predictions, key=lambda x: x[0])\n",
        "    return predictions\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGBcp6kju6zW"
      },
      "source": [
        "chur_tbl = pd.read_csv('churn.csv')"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qpwbsdbhr22_"
      },
      "source": [
        "chur_tbl = chur_tbl.drop([\"Surname\", \"CustomerId\", \"Geography\"], axis=1)\n",
        "\n",
        "mapping = {'Female': 1, 'Male' : 0}\n",
        "chur_tbl = chur_tbl.replace({'Gender': mapping})\n",
        "\n",
        "X = chur_tbl.iloc[:, 1:-1]\n",
        "y = chur_tbl[\"Exited\"]\n"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGh4OuUewdXs"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sO2AufVsRlT-"
      },
      "source": [
        "rf = RandomForest(trees_amount=5, max_depth=6)\n",
        "rf.fit(X_train, y_train)\n",
        "pred = rf.predict(X_test)\n",
        "ac_sc = accuracy_score(y_test, pred)\n"
      ],
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h7hGYMxHAqx"
      },
      "source": [
        "f_imp = rf.feature_importance(ac_sc, X_test, y_test)"
      ],
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5GYI4emIuze",
        "outputId": "c28adde0-436c-4347-b397-ec8dd754982c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        }
      },
      "source": [
        "pd.DataFrame(f_imp)\n",
        "# Here you can see how features builded forest depends on a certain features.\n",
        "# Lower the number –– higher the correlation"
      ],
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.071137</td>\n",
              "      <td>Age</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.043732</td>\n",
              "      <td>NumOfProducts</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.027405</td>\n",
              "      <td>IsActiveMember</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.004665</td>\n",
              "      <td>Balance</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.001749</td>\n",
              "      <td>CreditScore</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>-0.001749</td>\n",
              "      <td>Tenure</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-0.001166</td>\n",
              "      <td>EstimatedSalary</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>-0.000583</td>\n",
              "      <td>Gender</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.000583</td>\n",
              "      <td>HasCrCard</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          0                1\n",
              "0 -0.071137              Age\n",
              "1 -0.043732    NumOfProducts\n",
              "2 -0.027405   IsActiveMember\n",
              "3 -0.004665          Balance\n",
              "4 -0.001749      CreditScore\n",
              "5 -0.001749           Tenure\n",
              "6 -0.001166  EstimatedSalary\n",
              "7 -0.000583           Gender\n",
              "8  0.000583        HasCrCard"
            ]
          },
          "metadata": {},
          "execution_count": 230
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW7Fx8RcKvm2",
        "outputId": "67210158-e443-4693-ad3d-d742198122d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "params = {'trees_amount': [11, 13, 15, 20, 30, 40, 60, 80, 125],\n",
        "           'min_samples_split': [1.0, 2, 3, 4, 7, 8, 10, 13, 15, 18],\n",
        "           'max_depth': [3, 5, 6, 8, 9, 10]}\n",
        "best_ac = -1\n",
        "best_mod = None\n",
        "for t_a in params['trees_amount']:\n",
        "  for mss in params['min_samples_split']:\n",
        "    for md in params['max_depth']:\n",
        "      rf = RandomForest(trees_amount=t_a, max_depth=md, min_samples_split=mss)\n",
        "      rf.fit(X_train, y_train)\n",
        "      pred = rf.predict(X_test)\n",
        "      ac_sc = accuracy_score(y_test, pred)\n",
        "      if ac_sc > best_ac:\n",
        "        best_ac = ac_sc\n",
        "        best_mod = rf\n",
        "        best_params = {\n",
        "            'trees_amount' : t_a,\n",
        "            'min_samples_split' : mss,\n",
        "            'max_depth' : md\n",
        "        }\n",
        "print(best_ac, best_params)"
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.863 {'trees_amount': 30, 'min_samples_split': 18, 'max_depth': 8}\n"
          ]
        }
      ]
    }
  ]
}