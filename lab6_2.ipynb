{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test 6.2 neuron_activations.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkoop-MT9if-"
      },
      "source": [
        "# Нейрон с различными функциями активации"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUjxg_IS31tn"
      },
      "source": [
        "В этом задании нужно будет: \n",
        "- самостоятельно реализовать класс **`Neuron()`** с различными функциями активации (ReLU, LeakyReLU и ELU)\n",
        "\n",
        "- обучить и протестировать этот класс на сгенерированных и реальных данных\n",
        "\n",
        "**Достаточно реализовать ReLU и ещё одну из двух.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-OYlV519igB"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1VCt_rDHum3"
      },
      "source": [
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgpFOaPm9igG"
      },
      "source": [
        "В данном случае мы снова решаем задачу бинарной классификации (2 класса: 1 или 0). Мы уже выяснили (в ноутбуке про `logloss`), что плохо брать для классификации квадратичную функцию потерь, однако здесь для простоты возьмём её:\n",
        "\n",
        "$$\n",
        "Loss(\\hat{y}, y) = \\frac{1}{n}\\sum_{i=1}^{n} (\\hat{y_i} - y_i)^2\n",
        "$$  \n",
        "\n",
        "Здесь $w \\cdot X_i$ - скалярное произведение, а $\\hat{y_i} = \\sigma(w \\cdot X_i) =\\frac{1}{1+e^{-w \\cdot X_i}} $ - сигмоида ($i$ -- номер объекта в выборке). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn3F0layHunD"
      },
      "source": [
        "def Loss(y_pred, y):\n",
        "    y_pred = y_pred.reshape(-1, 1)\n",
        "    y = np.array(y).reshape(-1, 1)\n",
        "    return 0.5 * np.mean((y_pred - y) ** 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYoQAdXoHunK"
      },
      "source": [
        "Далее будут предложены несколько функций активации, и Вам нужно реализовать класс `Neuron` по аналогии с тем, как это было на семинаре. Сам принцип тот же, но меняются формула обновления весов и формула предсказания."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHvdwJW9HunN"
      },
      "source": [
        "<h2 style=\"text-align: center;\"><b>Нейрон с ReLU (Recitified Linear Unit)</b></h2>  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMA7Gi6KHunQ"
      },
      "source": [
        "ReLU самая часто используемая (по крайней мере, пару лет назад) функция активации в нейронных сетях. Выглядит она очень просто:\n",
        "\n",
        "\\begin{equation*}\n",
        "ReLU(x) =\n",
        " \\begin{cases}\n",
        "   0, &\\text{$x \\le 0$}\\\\\n",
        "   x, &\\text{$x \\gt 0$}\n",
        " \\end{cases}\n",
        "\\end{equation*}\n",
        "\n",
        "Или по-другому:\n",
        "\n",
        "$$\n",
        "ReLU(x) = \\max(0, x)\n",
        "$$\n",
        "\n",
        "В (свободном) переводе Rectified Linear Unit = \"Усечённая линейная функция\". Собственно, мы по сути просто не даём проходить отрицательным числам.\n",
        "\n",
        "Производная здесь берётся как производная от кусочно-заданной функции, то есть на участках, где функция гладкая, и в нуле её доопредляют нулём:\n",
        "\n",
        "\\begin{equation*}\n",
        "ReLU'(x) = \n",
        " \\begin{cases}\n",
        "   0, &\\text{$x \\le 0$}\\\\\n",
        "   1, &\\text{$x \\gt 0$}\n",
        " \\end{cases}\n",
        "\\end{equation*}\n",
        "\n",
        "Подставим ReLu в Loss:\n",
        "\n",
        "$$Loss(\\hat{y}, y) = \\frac{1}{2n}\\sum_{i=1}^{n} (\\hat{y_i} - y_i)^2 = \\frac{1}{2n}\\sum_{i=1}^{n} (ReLU(w \\cdot X_i) - y_i)^2 = \\begin{equation*}\n",
        "\\frac{1}{2n}\\sum_{i=1}^{n}\n",
        " \\begin{cases}\n",
        "    y_i^2, &{w \\cdot X_i \\le 0}\\\\\n",
        "   (w \\cdot X_i - y_i)^2, &{w \\cdot X_i \\gt 0}\n",
        " \\end{cases}\n",
        "\\end{equation*}$$  \n",
        "\n",
        "(помните, что $w \\cdot X_i$ -- это число в данном случае (результат скалярного произведения двух векторов)).\n",
        "\n",
        "Тогда формула для обновления весов при градиентном спуске будет такая (в матричном виде, рекмоендуем вывести самим то, как это получается из формулы для одного объекта):\n",
        "\n",
        "$$ \\frac{\\partial Loss}{\\partial w} = \\begin{equation*}\n",
        "\\frac{1}{n}\\sum_{i=1}^{n}\n",
        " \\begin{cases}\n",
        "   0, &{w \\cdot X_i \\le 0}\\\\\n",
        "   \\frac{1}{n} X_i^T (w \\cdot X_i - y), &{w \\cdot X_i \\gt 0}\n",
        " \\end{cases}\n",
        "\\end{equation*}$$\n",
        "\n",
        "(напоминаем, что здесь $w \\cdot X$ -- матричное произведение вектора $w$ (ведь вектор -- тоже матрица, не так ли?) и матрицы $X$ )\n",
        "\n",
        "Почему в первом случае будет 0? Потому что в формулу $y_i^2$ не входят веса , а мы берём производную именно по весам $w$.\n",
        "\n",
        "* Реализуйте ReLU и её производную:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCgAeho19igI"
      },
      "source": [
        "def relu(x):\n",
        "    \"\"\"ReLU-функция\"\"\"\n",
        "    return <Ваш код здесь>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXwsy-7J9igL"
      },
      "source": [
        "def relu_derivative(x):\n",
        "    \"\"\"Производная ReLU\"\"\"\n",
        "    return <Ваш код здесь>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKurn-7F9igN"
      },
      "source": [
        "Теперь нужно написать нейрон с ReLU. Здесь всё очень похоже на перцептрон, но будут по-другому обновляться веса и другая функция активации:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AM9vn3OX9igO"
      },
      "source": [
        "class NeuronReLU:\n",
        "    def __init__(self, w=None, b=0):\n",
        "        \"\"\"\n",
        "        :param: w -- вектор весов\n",
        "        :param: b -- смещение\n",
        "        \"\"\"\n",
        "        self.w = w\n",
        "        self.b = b\n",
        "        \n",
        "        \n",
        "    def activate(self, x):\n",
        "        <Ваш код здесь>\n",
        "        \n",
        "        \n",
        "    def forward_pass(self, X):\n",
        "        \"\"\"\n",
        "        Рассчитывает ответ нейрона при предъявлении набора объектов\n",
        "        :param: X -- матрица примеров размера (n, m), каждая строка - отдельный объект\n",
        "        :return: вектор размера (n, 1) из нулей и единиц с ответами нейрона \n",
        "        \"\"\"\n",
        "        n = X.shape[0]\n",
        "        y_pred = np.zeros((n, 1))  # y_pred == y_predicted - предсказанные классы\n",
        "        <Ваш код здесь>\n",
        "    \n",
        "    \n",
        "    def backward_pass(self, X, y, y_pred, learning_rate=0.005):\n",
        "        \"\"\"\n",
        "        Обновляет значения весов нейрона в соответствии с этим объектом\n",
        "        :param: X -- матрица входов размера (n, m)\n",
        "                y -- вектор правильных ответов размера (n, 1)\n",
        "                learning_rate - \"скорость обучения\" (символ alpha в формулах выше)\n",
        "        В этом методе ничего возвращать не нужно, только правильно поменять веса\n",
        "        с помощью градиентного спуска.\n",
        "        \"\"\"\n",
        "        n = len(y)\n",
        "        y = np.array(y).reshape(-1, 1)\n",
        "        <Ваш код здесь>\n",
        "        \n",
        "    \n",
        "    def fit(self, X, y, num_epochs=300):\n",
        "        \"\"\"\n",
        "        Спускаемся в минимум\n",
        "        :param: X -- матрица объектов размера (n, m)\n",
        "                y -- вектор правильных ответов размера (n, 1)\n",
        "                num_epochs -- количество итераций обучения\n",
        "        :return: losses -- вектор значений функции потерь\n",
        "        \"\"\"\n",
        "        self.w = np.zeros((X.shape[1], 1))  # столбец (m, 1)\n",
        "        self.b = 0  # смещение (число)\n",
        "        Loss_values = []  # значения функции потерь на различных итерациях обновления весов\n",
        "        \n",
        "        for i in range(num_epochs):\n",
        "            <Ваш код здесь>\n",
        "        \n",
        "        return Loss_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thtFp-at9igS"
      },
      "source": [
        "<h3 style=\"text-align: center;\"><b>Тестирование нейрона с ReLU</b></h3>  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOuYzf_u9igS"
      },
      "source": [
        "Здесь вам нужно самим протестировать новый нейрон **на тех же данных** (\"Яблоки и Груши\" и \"Голос\") по аналогии с тем, как это было проделано с перцептроном.\n",
        "В итоге нужно вывести: \n",
        "* график, на котором будет показано, как изменяется функция потерь $Loss$ в зависимости от числа итераций обучения\n",
        "* график с раскраской выборки нейроном"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chEeb88gHuny"
      },
      "source": [
        "***ПРИМЕЧАНИЕ***: пожалуйста, почаще проверяйте `.shape` у матриц и векторов: `self.w`, `X` и `y` внутри класса. Очень часто ошибка решается транспонированием или `.reshape()`'ом. Не забывайте проверять, что на что Вы умножаете и какой вектор (какой размер) хотите получить на выходе -- это очень помогает не запутаться."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZDtNG6CHuop"
      },
      "source": [
        "%%time\n",
        "\n",
        "neuron = <Ваш код здесь>\n",
        "Loss_values = <Ваш код здесь>\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(Loss_values)\n",
        "plt.title('Функция потерь', fontsize=15)\n",
        "plt.xlabel('номер итерации', fontsize=14)\n",
        "plt.ylabel('$Loss(\\hat{y}, y)$', fontsize=14)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPxo2YVeHuou"
      },
      "source": [
        "Скорее всего сейчас у вас лосс -- это прямая линия, и вы видите, что веса не обновляются. Но почему?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXUmEeUBHuov"
      },
      "source": [
        "Всё просто -- если присмотреться, то видно, что self.w и self.b иницилизируются нулями в начале `.fit()`-метода. Если расписать, как будет идти обновление, то видно, что из-за ReLU веса просто-напросто не будут обновляться, если начать с инициализации нулями. \n",
        "\n",
        "Это -- одна из причин, по которой в нейронных сетях веса инициализируют случаными числами (обычно из отрезка [0, 1)).\n",
        "\n",
        "Обучите нейрон, инициализировав случайно веса (поставьте 10000 итераций). \n",
        "\n",
        "Закомментируйте инициализацию нулями в функции `.fit()` класса `NeuronReLU` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E821cUM0Huo8"
      },
      "source": [
        "%%time\n",
        "\n",
        "neuron = NeuronReLU(w=np.random.rand(X.shape[1], 1), b=np.random.rand(1))\n",
        "Loss_values = neuron.fit(X, y, num_epochs=10000)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(Loss_values)\n",
        "plt.title('Функция потерь', fontsize=15)\n",
        "plt.xlabel('номер итерации', fontsize=14)\n",
        "plt.ylabel('$Loss(\\hat{y}, y)$', fontsize=14)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkWMrc9uHupS"
      },
      "source": [
        "Посмотрим, как предсказывает этот нейрон:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyjfcthcHupT",
        "scrolled": false
      },
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(data.iloc[:, 0], data.iloc[:, 1], c=np.array(neuron.forward_pass(X) > 0.5).ravel(), cmap='spring')\n",
        "plt.title('Яблоки и груши', fontsize=15)\n",
        "plt.xlabel('симметричность', fontsize=14)\n",
        "plt.ylabel('желтизна', fontsize=14)\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSNZuWl9Hupd"
      },
      "source": [
        "Есть одна тенденция: пороговая функция активации и сигмоида (обычно всё же только сигмоида) чаще используются именно на **выходном слое** нейросети в задаче классификации -- ими предсказывают вероятности объектов принадлежать одному из классов, в то время как продвинутые функции активации (ReLU и те, что будут дальше) используются внутри нейросети, то есть в **скрытых слоях**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_X3lDHElHupo"
      },
      "source": [
        "Нужно понимать, что ReLU не может вернуть отрицательные числа."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sET710OVHupp"
      },
      "source": [
        "**Плюсы ReLU:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HY3iXGBWHupr"
      },
      "source": [
        "* дифференцируемая (с доопределе\n",
        "нием в нуле)\n",
        "* нет проблемы затухающих градиентов, как в сигмоиде"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nr_3XwTWHups"
      },
      "source": [
        "**Возможные минусы ReLU:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TAwVdRXHupt"
      },
      "source": [
        "* не центрирована около 0 (может мешать скорости сходимсти)\n",
        "* зануляет все отрицательные входы, тем самым веса у занулённых нейронов могут часто *не обновляться*, эту проблему иногда называют *мёртвые нейроны*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vj1JGXTPHupu"
      },
      "source": [
        "С последней проблемой можно побороться, а именно:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yn2taDMNHupv"
      },
      "source": [
        "<h2 style=\"text-align: center;\"><b>Нейрон с LeakyReLU (Leaky Recitified Linear Unit)</b></h2>  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCRBWooSHupx"
      },
      "source": [
        "LeakyReLU очень слабо отличается от ReLU, но часто помогает сети обучаться быстрее, поскольку нет проблемы \"мёртвых нейронов\":\n",
        "\n",
        "\\begin{equation*}\n",
        "LeakyReLU(x) =\n",
        " \\begin{cases}\n",
        "   \\alpha x, &\\text{$x \\le 0$}\\\\\n",
        "   x, &\\text{$x \\gt 0$}\n",
        " \\end{cases}\n",
        "\\end{equation*}\n",
        "\n",
        "где $\\alpha$ -- маленькое число от 0 до 1.\n",
        "\n",
        "Производная здесь берётся так же, но вместо нуля будет $\\alpha$:\n",
        "\n",
        "\\begin{equation*}\n",
        "LeakyReLU'(x) = \n",
        " \\begin{cases}\n",
        "   \\alpha, &\\text{$x \\le 0$}\\\\\n",
        "   1, &\\text{$x \\gt 0$}\n",
        " \\end{cases}\n",
        "\\end{equation*}\n",
        "\n",
        "График этой функции:\n",
        "\n",
        "<img src=\"https://cdn-images-1.medium.com/max/1600/0*UtLlZJ80TMIM7kXk.\" width=400 height=300>\n",
        "\n",
        "Подставим LeakyReLu в Loss:\n",
        "\n",
        "$$\n",
        "Loss(\\hat{y}, y) = \\frac{1}{2n}\\sum_{i=1}^{n} (\\hat{y_i} - y_i)^2 = \\frac{1}{2n}\\sum_{i=1}^{n} (LeakyReLU(w \\cdot X_i) - y_i)^2 =\n",
        "\\begin{equation*}\n",
        "\\frac{1}{2n}\\sum_{i=1}^{n} \n",
        " \\begin{cases}\n",
        "   (\\alpha \\cdot w \\cdot X_i - y_i)^2, &{w \\cdot X_i \\le 0}\\\\\n",
        "   (w \\cdot X_i - y_i)^2, &{w \\cdot X_i \\gt 0}\n",
        " \\end{cases}\n",
        "\\end{equation*}\n",
        "$$  \n",
        "\n",
        "Формула для обновления весов при градиентном спуске:\n",
        "\n",
        "$$ \\frac{\\partial Loss}{\\partial w} = \\begin{equation*}\n",
        "\\frac{1}{n}\\sum_{i=1}^{n} \n",
        " \\begin{cases}\n",
        "   \\alpha X_i^T (w \\cdot X_i - y), &{w \\cdot X_i \\le 0}\\\\\n",
        "    X_i^T (w \\cdot X_i - y), &{w \\cdot X_i \\gt 0}\n",
        " \\end{cases}\n",
        "\\end{equation*}$$\n",
        "\n",
        "* Реализуйте LeakyReLU и её производную:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvvL-J5Tg733"
      },
      "source": [
        "def leaky_relu(x, alpha=0.01):\n",
        "    \"\"\"LeakyReLU-функция\"\"\"\n",
        "    <Ваш код здесь>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "An5RxMX_g736"
      },
      "source": [
        "def leaky_relu_derivative(x, alpha=0.01):\n",
        "    \"\"\"Производная LeakyReLU\"\"\"\n",
        "    <Ваш код здесь>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pVZT9RZg738"
      },
      "source": [
        "Теперь нужно написать нейрон с LeakyReLU функцией активации. Здесь всё очень похоже на перцептрон, но будут по-другому обновляться веса и другая функция активации:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UM7qpfxPg739"
      },
      "source": [
        "class NeuronLeakyReLU:\n",
        "    def __init__(self, w=None, b=0):\n",
        "        \"\"\"\n",
        "        :param: w -- вектор весов\n",
        "        :param: b -- смещение\n",
        "        \"\"\"\n",
        "        self.w = w\n",
        "        self.b = b\n",
        "        \n",
        "        \n",
        "    def activate(self, x):\n",
        "        <Ваш код здесь>\n",
        "        \n",
        "        \n",
        "    def forward_pass(self, X):\n",
        "        \"\"\"\n",
        "        Рассчитывает ответ нейрона при предъявлении набора объектов\n",
        "        :param: X -- матрица примеров размера (n, m), каждая строка - отдельный объект\n",
        "        :return: вектор размера (n, 1) из нулей и единиц с ответами нейрона \n",
        "        \"\"\"\n",
        "        n = X.shape[0]\n",
        "        y_pred = np.zeros((n, 1))  # y_pred == y_predicted - предсказанные классы\n",
        "        <Ваш код здесь>\n",
        "        \n",
        "    \n",
        "    def backward_pass(self, X, y, y_pred, learning_rate=0.005):\n",
        "        \"\"\"\n",
        "        Обновляет значения весов нейрона в соответствии с этим объектом\n",
        "        :param: X -- матрица входов размера (n, m)\n",
        "                y -- вектор правильных ответов размера (n, 1)\n",
        "                learning_rate - \"скорость обучения\" (символ alpha в формулах выше)\n",
        "        В этом методе ничего возвращать не нужно, только правильно поменять веса\n",
        "        с помощью градиентного спуска.\n",
        "        \"\"\"\n",
        "        n = len(y)\n",
        "        y = np.array(y).reshape(-1, 1)\n",
        "        <Ваш код здесь>\n",
        "    \n",
        "    \n",
        "    def fit(self, X, y, num_epochs=300):\n",
        "        \"\"\"\n",
        "        Спускаемся в минимум\n",
        "        :param: X -- матрица объектов размера (n, m)\n",
        "                y -- вектор правильных ответов размера (n, 1)\n",
        "                num_epochs -- количество итераций обучения\n",
        "        :return: losses -- вектор значений функции потерь\n",
        "        \"\"\"\n",
        "#         self.w = np.zeros((X.shape[1], 1))  # столбец (m, 1)\n",
        "#         self.b = 0  # смещение (число)\n",
        "        Loss_values = []  # значения функции потерь на различных итерациях обновления весов\n",
        "        \n",
        "        for i in range(num_epochs):\n",
        "            <Ваш код здесь>\n",
        "        \n",
        "        return Loss_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgTac_vFHuq8"
      },
      "source": [
        "**Плюсы LeakyReLU:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAw-tzuLHuq9"
      },
      "source": [
        "* дифференцируемая (с доопределнием в нуле)\n",
        "* нет проблемы затухающих градиентов, как в сигмоиде\n",
        "* нет проблемы \"мёртвых нейронов\", как в ReLU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMUGymwyHuq_"
      },
      "source": [
        "**Возможные минусы LeakyReLU:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQyqD8-LHurA"
      },
      "source": [
        "* не центрирована около 0 (может мешать скорости сходимсти)\n",
        "* немного не устойчива к \"шуму\" (см. лекции Стэнфорда)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7k07EGyHurB"
      },
      "source": [
        "<h2 style=\"text-align: center;\"><b>Нейрон с ELU (Exponential Linear Unit)</a></b></h2>  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWH3Zk2zHurB"
      },
      "source": [
        "ELU -- не так давно предложенная (в 2015 году) функция активации, которая, как говорят авторы статьи, лучше LeakyReLU. Вот формула ELU:\n",
        "\n",
        "\\begin{equation*}\n",
        "ELU(\\alpha, x) =\n",
        " \\begin{cases}\n",
        "   \\alpha (e^x - 1), &\\text{$x \\le 0$}\\\\\n",
        "   x, &\\text{$x \\gt 0$}\n",
        " \\end{cases}\n",
        "\\end{equation*}\n",
        "\n",
        "где $\\alpha$ -- маленькое число от 0 до 1.\n",
        "\n",
        "Производная здесь берётся так же, но вместо нуля будет $\\alpha$:\n",
        "\n",
        "\\begin{equation*}\n",
        "ELU'(x) = \n",
        " \\begin{cases}\n",
        "   ELU(\\alpha, x) + \\alpha, &\\text{$x \\le 0$}\\\\\n",
        "   1, &\\text{$x \\gt 0$}\n",
        " \\end{cases}\n",
        "\\end{equation*}\n",
        "\n",
        "Здесь в производной использован постой трюк -- сделано $- \\alpha + \\alpha$, чтобы вычислять было проще.\n",
        "\n",
        "График этой функции:\n",
        "\n",
        "<img src=\"http://p0.ifengimg.com/pmop/2017/0907/A004001DD141881BFD8AD62E5D31028C3BE3FAD1_size14_w446_h354.png\" width=500 height=400>\n",
        "\n",
        "Подставим LeakyReLu в Loss:\n",
        "\n",
        "$$Loss(\\hat{y}, y) = \\frac{1}{2n}\\sum_{i=1}^{n} (\\hat{y_i} - y_i)^2 = \\frac{1}{2n}\\sum_{i=1}^{n} (ELU(\\alpha, w \\cdot X_i) - y_i)^2 = \\begin{equation*}\n",
        "\\frac{1}{2n}\\sum_{i=1}^{n} \n",
        " \\begin{cases}\n",
        "   (\\alpha (e^{w \\cdot X_i} - 1) - y_i)^2, &{w \\cdot X_i \\le 0}\\\\\n",
        "   (w \\cdot X_i - y_i)^2, &{w \\cdot X_i \\gt 0}\n",
        " \\end{cases}\n",
        "\\end{equation*}$$  \n",
        "\n",
        "Здесь вам нужно выписать самим град спуск для весов. Брать производную \"в лоб\" некрасиво и неудобно. Нужно воспользоваться **правилом цепочки**, оно же **правило взятия производной сложной функции**:\n",
        "\n",
        "$$ \\frac{\\partial Loss}{\\partial w} = \\begin{equation*}\n",
        "\\frac{1}{n}\\sum_{i=1}^{n} \n",
        " \\begin{cases}\n",
        "   , &{w \\cdot X_i \\le 0}\\\\\n",
        "   , &{w \\cdot X_i \\gt 0}\n",
        " \\end{cases}\n",
        "\\end{equation*}$$\n",
        "\n",
        "* Реализуйте ELU и её производную:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mc6fsB5HzR2Q"
      },
      "source": [
        "def eelu(x, alpha=0.01):\n",
        "    \"\"\"LeakyReLU-функция\"\"\"\n",
        "    <Ваш код здесь>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMSIjprszR2T"
      },
      "source": [
        "def elu_derivative(x, alpha=0.01):\n",
        "    \"\"\"Производная LeakyReLU\"\"\"\n",
        "    <Ваш код здесь>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5p1eDNGzR2V"
      },
      "source": [
        "Теперь нужно написать нейрон с LeakyReLU функцией активации. Здесь всё очень похоже на перцептрон, но будут по-другому обновляться веса и другая функция активации:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cxpveqozR2X"
      },
      "source": [
        "class NeuronELU:\n",
        "    def __init__(self, w=None, b=0):\n",
        "        \"\"\"\n",
        "        :param: w -- вектор весов\n",
        "        :param: b -- смещение\n",
        "        \"\"\"\n",
        "        self.w = w\n",
        "        self.b = b\n",
        "        \n",
        "        \n",
        "    def activate(self, x):\n",
        "        <Ваш код здесь>\n",
        "        \n",
        "        \n",
        "    def forward_pass(self, X):\n",
        "        \"\"\"\n",
        "        Рассчитывает ответ нейрона при предъявлении набора объектов\n",
        "        :param: X -- матрица примеров размера (n, m), каждая строка - отдельный объект\n",
        "        :return: вектор размера (n, 1) из нулей и единиц с ответами нейрона \n",
        "        \"\"\"\n",
        "        n = X.shape[0]\n",
        "        y_pred = np.zeros((n, 1))  # y_pred == y_predicted - предсказанные классы\n",
        "        <Ваш код здесь>\n",
        "        \n",
        "    \n",
        "    def backward_pass(self, X, y, y_pred, learning_rate=0.005):\n",
        "        \"\"\"\n",
        "        Обновляет значения весов нейрона в соответствии с этим объектом\n",
        "        :param: X -- матрица входов размера (n, m)\n",
        "                y -- вектор правильных ответов размера (n, 1)\n",
        "                learning_rate - \"скорость обучения\" (символ alpha в формулах выше)\n",
        "        В этом методе ничего возвращать не нужно, только правильно поменять веса\n",
        "        с помощью градиентного спуска.\n",
        "        \"\"\"\n",
        "        n = len(y)\n",
        "        y = np.array(y).reshape(-1, 1)\n",
        "        <Ваш код здесь>\n",
        "    \n",
        "    \n",
        "    def fit(self, X, y, num_epochs=300):\n",
        "        \"\"\"\n",
        "        Спускаемся в минимум\n",
        "        :param: X -- матрица объектов размера (n, m)\n",
        "                y -- вектор правильных ответов размера (n, 1)\n",
        "                num_epochs -- количество итераций обучения\n",
        "        :return: losses -- вектор значений функции потерь\n",
        "        \"\"\"\n",
        "#         self.w = np.zeros((X.shape[1], 1))  # столбец (m, 1)\n",
        "#         self.b = 0  # смещение (число)\n",
        "        Loss_values = []  # значения функции потерь на различных итерациях обновления весов\n",
        "        \n",
        "        for i in range(num_epochs):\n",
        "            <Ваш код здесь>\n",
        "        \n",
        "        return Loss_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYiGj1wnHur4"
      },
      "source": [
        "**Плюсы ELU:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOGAh06uHur5"
      },
      "source": [
        "* дифференцируемая (с доопределнием в нуле)\n",
        "* нет проблемы затухающих градиентов, как в сигмоиде\n",
        "* нет проблемы \"мёртвых нейронов\", как в ReLU\n",
        "* более устойчива к \"шуму\" (см. лекции Стэнфорда)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4doD89fHur6"
      },
      "source": [
        "**Возможные минусы ELU:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AO4bvkKGHur6"
      },
      "source": [
        "* не очень хорошо центрирована около 0 (может мешать скорости сходимсти)\n",
        "* вычислительно дольше, чем ReLU и LeakyReLU"
      ]
    }
  ]
}